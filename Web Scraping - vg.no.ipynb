{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the stuff that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import requests\n",
    "import json\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve URL's to the individual news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"Selenium\" package is used to open the web page. The reason for this is that the page listing the articles is javascript based, and the javascript is used to load the article. If you would not accessed this page directly from BeautifulSoup, the aricles would not have been loaded.\n",
    "\n",
    "Note that we also need to scroll down on the web page to get more articles. For this purpose, selenium can also be used, as selenium has functionality for all kinds of intreraction with the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_urls(topic):\n",
    "    # Create the browser and access the page\n",
    "    url = \"http://www.vg.no/nyheter/\" + topic\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    \n",
    "    # Scroll down to get more news articles\n",
    "    for i in range(10):\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "    # Fetch all the visible article urls\n",
    "    article_urls = []\n",
    "    for article in browser.find_elements_by_tag_name(\"article\"):\n",
    "        a_url = article.find_element_by_tag_name(\"a\").get_attribute(\"href\")\n",
    "        if a_url not in article_urls:\n",
    "            article_urls.append(a_url)\n",
    "            \n",
    "    browser.close()\n",
    "    print(\"{} articles about {} found\".format(len(article_urls), topic))\n",
    "    return article_urls    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the article body\n",
    "\n",
    "The article body consists of three different types of text (on this page):\n",
    "- **The preamble** - the introduction to the article\n",
    "- **Headers**\n",
    "- **Paragraphs** - the main content\n",
    "\n",
    "We need to exract each of these parts and then concatenate them. Note that each part has substrings, so this is why we need to iterate through all the `strings` of each text element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_article_body(soup):\n",
    "    # Concatenate the different parts of the text\n",
    "    text = \"\"\n",
    "    article_body = soup.find(\"div\", {\"itemprop\": \"articleBody\"})\n",
    "    for c in article_body.children:\n",
    "        if type(c) == bs4.element.Tag:\n",
    "            # Read only the preamble, paragraphs and headers\n",
    "            if c.get(\"id\") == \"preamble\" or c.name == 'p' or c.name[0] == \"h\":\n",
    "                for s in c.strings:\n",
    "                    text += s + \" \"\n",
    "                    \n",
    "    # Some text cleaning\n",
    "    text = re.sub(\" (?=[.!?])\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "    text = re.sub(\"(?<=[.!?]) â€“(?= )\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving an article\n",
    "\n",
    "In this retrieval method, we retrieve the following parts of the article:\n",
    "- The **author** of the article\n",
    "- The **time** the article was created\n",
    "- The **title** of the article\n",
    "- The **body** (main text) of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_article(url):\n",
    "    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "    article = {}\n",
    "    \n",
    "    # Get the auther of the article\n",
    "    try:\n",
    "        article[\"author\"] = soup.find(\"a\", {\"itemprop\": \"author\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        article[\"author\"] = None\n",
    "    \n",
    "    # Creation time\n",
    "    try:\n",
    "        article[\"create_time\"] = soup.find(\"time\", {\"itemprop\": \"datePublished\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        article[\"create_time\"] = None\n",
    "    \n",
    "    # Get the title\n",
    "    article[\"title\"] = soup.find(\"h1\", class_=\"main-title\").get_text().strip()\n",
    "    \n",
    "    # Get the body\n",
    "    article[\"body\"] = extract_article_body(soup).strip()\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting some articles for specific topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 articles about Donald Trump found\n",
      "140 articles about fotball found\n"
     ]
    }
   ],
   "source": [
    "topics = [\"Donald Trump\", \"fotball\"]\n",
    "articles = []\n",
    "for topic in topics:\n",
    "    for url in get_article_urls(topic):\n",
    "        article = retrieve_article(url)\n",
    "        article[\"topic\"] = topic\n",
    "        articles.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "    \n",
    "with open(os.path.join(\"data\", \"vg_articles.csv\"), 'w', encoding='utf-8') as csvfile:\n",
    "    columns = [\"author\", \"create_time\", \"title\", \"body\", \"topic\"]\n",
    "    csv_writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    csv_writer.writeheader()\n",
    "    for row in articles:\n",
    "        csv_writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
